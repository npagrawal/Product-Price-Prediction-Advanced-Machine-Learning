{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b798f55",
   "metadata": {},
   "source": [
    "# Price Prediction For Skincare Products Using Neural Networks\n",
    "\n",
    "## Overview\n",
    "\n",
    "The cosmetics industry brought in an estimated $262.21 billion in 2022 and is due to continue growing this year. Skincare in particular has seen an increased interest as the culture has shifted away from the glam beauty standards of the recession-era 2000s to embrace a more \"natural\" beauty. \n",
    "\n",
    "My stakeholder, Inner Glow Inc., wants to make their mark on the skincare industry by coming out with their own line of products that are competetive with what's available. To do this, they first need to understand the current market and determine where they can undercut more expensive alternatives. Rampant upcharge in the skincare industry is obvious when products can range from $7-400. How can Inner Glow Inc. offer mid-line products that capture both a high-end and low-end market?\n",
    "\n",
    "Using neural networks and XGBoost, I look at data from NoxMoon's \"Inside Beauty\" project, scraped from Beautypedia and Paula’s choice websites, and data I scraped myself from the Dermstore. The two datasets include product names, brands, prices, ingredients, size, and product type. There are over 7000 products at the end of cleaning. Features include one-hot encoded \"special\" ingredients (i.e. not ones that appear in every single product but do in a good amount of them), and number of active and inactive ingredients, among others.\n",
    "\n",
    "After tuning a series of neural networks, using XGBoost, and using grid search to tune XGBoost, I find in the end that the untuned XGBoost does the best with the data, scoring an MAE on the test around $13. Given the spread of product prices, this doesn't seem way off base. Features engineering determines certain ingredients, like Isopropyl Misorate, have a marginal level of importance when it comes to price. The most often overpriced brands are iS Clinical, Clarins, and SK-II, while the most underpriced ones are Neutrogena, Clinique, and L'Oreal Paris. Therefore, Inner Beauty Inc. should try to dupe the overpriced brands' products with prices that are on average higher than the underpriced ones.\n",
    "\n",
    "I put all these findings together into the Streamlit app SuperDuper which offers users the chance to dupe their favorite high-end products and find cheaper alternatives that are at the ingredient-level similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a40085e",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "To begin, we import all our libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a25a4474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.losses import MeanAbsoluteError\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475789fe",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "The data comes from Beautypedia and Paula's Choice (scraped by NoxMoon on GitHub) and from the Dermstore. You can see my scraping code for the Dermstore in the web_scraping folder. Both datasets include product names, prices, categories, brands, and ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8010ee10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read NoxMoon's data\n",
    "df1 = pd.read_csv(\"data/skin_care_cleaned.csv\")\n",
    "df1 = df1[[\"product_names\", \"product_category\", \"brand\", \"ingredient\", \"size\", \"price\", \"size_num\", \"size_unit\", \n",
    "        'active_ingredient', 'inactive_ingredient', 'n_inactive_ingredient', 'n_active_ingredient']]\n",
    "\n",
    "# basic cleaning for readability, dropping empty cells\n",
    "df1.active_ingredient = df1.active_ingredient.str.replace(\"(\", \"\")\n",
    "df1.active_ingredient = df1.active_ingredient.str.replace(\")\", \"\")\n",
    "df1.active_ingredient = df1.active_ingredient.str.replace(\",\", \"\")\n",
    "df1.active_ingredient = df1.active_ingredient.str.replace(\"%.\", \"%\")\n",
    "df1.active_ingredient = df1.active_ingredient.str.strip()\n",
    "df1.active_ingredient = df1.active_ingredient.str.split(\"%\")\n",
    "df1 = df1.dropna(subset=[\"ingredient\", \"size\", \"size_unit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d84fe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0744677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in Dermstore data\n",
    "df2 = pd.read_csv(\"data/ingredients_skincare_dermstore.csv\").drop(columns = [\"Unnamed: 0\", \"url\"])\n",
    "\n",
    "# remove dollar signs from dataframe\n",
    "df2[\"price\"] = df2[\"price\"].str.replace(\"$\", \"\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edff50d",
   "metadata": {},
   "source": [
    "Both datasets seem to be missing data for active ingredients, and the second dataset is missing data for inactive ingredients. This might be because not all products have active and inactive ingredient distinctions.\n",
    "\n",
    "Let's look at the range of prices. First we'll need to remove rows where the price data isn't a number. And then we'll need to convert the prices to floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0b715d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find and drop irregular rows\n",
    "index_list = []\n",
    "for index, row in df2.iterrows():\n",
    "    if \"-product\" in str(row[\"price\"]):\n",
    "        index_list.append(index)\n",
    "    elif \"-qubit\" in str(row[\"price\"]):\n",
    "        index_list.append(index)\n",
    "    elif \"-track\" in str(row[\"price\"]):\n",
    "        index_list.append(index)\n",
    "        \n",
    "df2 = df2.drop(index = index_list)\n",
    "\n",
    "# convert price column to float\n",
    "df2[\"price\"] = df2[\"price\"].astype(float)\n",
    "\n",
    "# put the dataframes together\n",
    "df = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44c73f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the prices by product category\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "sns.boxplot(x='product_category', y='price', data=df, ax=axs[0])\n",
    "axs[0].set_xticklabels(axs[0].get_xticklabels(), rotation=90)\n",
    "axs[0].set_xlabel(\"Product Category\")\n",
    "axs[0].set_ylabel(\"Price\")\n",
    "axs[0].set_title(\"Product Categories by Price\")\n",
    "\n",
    "sns.boxplot(x='product_category', y='price', data=df, ax=axs[1])\n",
    "axs[1].set_xticklabels(axs[1].get_xticklabels(), rotation=90)\n",
    "axs[1].set_xlabel(\"Product Category\")\n",
    "axs[1].set_ylabel(\"Price\")\n",
    "axs[1].set_title(\"Product Categories by Price, Closeup\")\n",
    "axs[1].set_ylim(0, 200);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb3620",
   "metadata": {},
   "source": [
    "So, the majority of products average less than $50, while serums tick higher than the others. There are probably more designer products in the serum category than the others. Lip balms and scrubs seem to be the cheapest items. There are also many outliers that tick even higher than these averages (left plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e70e196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select most common brands\n",
    "brand_list = []\n",
    "for key, value in df.brand.value_counts().head(16).to_dict().items():\n",
    "    brand_list.append(key)\n",
    "\n",
    "df_topbrands = df[df[\"brand\"].isin(brand_list)]\n",
    "df_topbrands = df_topbrands[[\"brand\", \"price\", \"product_names\"]]\n",
    "df_topbrands = df_topbrands.dropna()\n",
    "\n",
    "# plot brands against price\n",
    "ax = sns.boxplot(x='brand', y='price', data=df_topbrands)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "ax.set_xlabel(\"Brand\")\n",
    "ax.set_ylabel(\"Price\")\n",
    "ax.set_title(\"Brands by Price\")\n",
    "ax.set_ylim(0, 200);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7f10b3",
   "metadata": {},
   "source": [
    "Lancome has tthe greatest spread of prices amongst its catalog, while Neutrogena has the smallest. Neutrogena also seems to be the cheapest.\n",
    "\n",
    "Since brands and product categories seem to be a great determinant of price, there's a good chance both will show up in the features engineering portion of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec91a7a1",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "In order to seamlessly combine the datasets, we'll need to do some cleaning. We'll go back to dfs 1 and 2 to do this. It'll also help us to turn our ingredient columns into lists for one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a03f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some cleaning\n",
    "# turn active ingredients into list, input inactive ingredients\n",
    "df2[\"active_ingredients\"] = df2[\"active_ingredients\"].str.split(\",\")\n",
    "df2[\"inactive_ingredients\"] = df2[\"inactive_ingredients\"].fillna(df2[\"ingredients\"])\n",
    "\n",
    "# create and fill n_active_ingredient column\n",
    "df2[\"n_active_ingredient\"] = df2[\"active_ingredients\"]\n",
    "df2[\"n_active_ingredient\"] = df2[\"n_active_ingredient\"].fillna(0)\n",
    "for index, row in df2.iterrows():\n",
    "    if type(row[\"n_active_ingredient\"]) == list:\n",
    "        if len(row[\"n_active_ingredient\"]) > 0:\n",
    "            row[\"n_active_ingredient\"] = len(row[\"n_active_ingredient\"])\n",
    "            \n",
    "# create and fill n_inactive_ingredient column\n",
    "df2[\"n_inactive_ingredient\"] = df2[\"inactive_ingredients\"].str.split(\",\")\n",
    "for index, row in df2.iterrows():\n",
    "    if type(row[\"n_inactive_ingredient\"]) == list:\n",
    "        if len(row[\"n_inactive_ingredient\"]) > 0:\n",
    "            row[\"n_inactive_ingredient\"] = len(row[\"n_inactive_ingredient\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a517c5f",
   "metadata": {},
   "source": [
    "Df2 doesn't have product categories, like df1 does. We'll have to semi-manually create these based on keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58a2fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create product categories for items\n",
    "df2[\"product_category\"] = df2[\"product_name\"]\n",
    "for index, row in df2.iterrows():\n",
    "    if \"Serum\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Serum\"\n",
    "    elif \"Hydr\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Serum\"\n",
    "    elif \"Refi\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Serum\"\n",
    "    elif \"Bar\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Serum\"\n",
    "    elif \"Stem\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Serum\"\n",
    "    elif \"Neck\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Serum\"\n",
    "    elif \"neck\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Serum\"\n",
    "    elif \"Peptide\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Serum\"\n",
    "    elif \"Resurfac\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Serum\"\n",
    "    elif \"Collagen\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Serum\"\n",
    "    elif \"Sun\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Sunscreen\" \n",
    "    elif \"SPF\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Sunscreen\" \n",
    "    elif \"Clean\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Cleansers\"\n",
    "    elif \"Wash\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Cleansers\"\n",
    "    elif \"Wipes\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Cleansers\"\n",
    "    elif \"Pads\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Cleansers\" \n",
    "    elif \"Makeup Remover\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Cleansers\"\n",
    "    elif \"Soap\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Cleansers\" \n",
    "    elif \"Cream\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Nighttime Moisturizer\"\n",
    "    elif \"Creme\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Nighttime Moisturizer\"\n",
    "    elif \"Night\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Nighttime Moisturizer\"\n",
    "    elif \"night\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Nighttime Moisturizer\"\n",
    "    elif \"Nuit\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Nighttime Moisturizer\"\n",
    "    elif \"Gel\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Nighttime Moisturizer\"\n",
    "    elif \"Sleep\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Nighttime Moisturizer\"\n",
    "    elif \"Lotion\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Daytime Moisturizer\"\n",
    "    elif \"Moisturizer\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Daytime Moisturizer\"\n",
    "    elif \"Butter\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Daytime Moisturizer\"\n",
    "    elif \"Lip\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Lip Scrub\"\n",
    "    elif \"Mask\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Face Mask\"\n",
    "    elif \"Masque\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Face Mask\"\n",
    "    elif \"Acne\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Acne & Blemish Treatment\"\n",
    "    elif \"Blemish\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Acne & Blemish Treatment\"\n",
    "    elif \"Pore\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Acne & Blemish Treatment\"\n",
    "    elif \"Spot\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Acne & Blemish Treatment\"\n",
    "    elif \"Clinical\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Acne & Blemish Treatment\"\n",
    "    elif \"System\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Acne & Blemish Treatment\"\n",
    "    elif \"Treatment\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Acne & Blemish Treatment\"\n",
    "    elif \"Eye\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Eye Cream & Treatment\" \n",
    "    elif \"Toner\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Toner & Face Mist\"\n",
    "    elif \"Mist\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Toner & Face Mist\"\n",
    "    elif \"mist\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Toner & Face Mist\" \n",
    "    elif \"Scrub\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Exfoliants\"\n",
    "    elif \"Exfoli\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Exfoliants\" \n",
    "    elif \"Polish\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Exfoliants\"\n",
    "    elif \"Glow\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Exfoliants\"\n",
    "    elif \"Peel\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Exfoliants\"\n",
    "    elif \"Vitamin C\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Vitamin C\"\n",
    "    elif \"Vita C\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Vitamin C\"\n",
    "    elif \"Refin\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Vitamin C\"\n",
    "    elif \"Bright\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Skin Lightener\"\n",
    "    elif \"-C\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Vitamin C\"\n",
    "    elif \"Vit C\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Vitamin C\"\n",
    "    elif \"- C\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Vitamin C\"\n",
    "    elif \"Oil\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Facial Oil\" \n",
    "    elif \"Retinol\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Retinol\" \n",
    "    elif \"Matt\" in row[\"product_category\"]:\n",
    "        row[\"product_category\"] = \"Oil Control Products\" \n",
    "        \n",
    "product_types = [\"Nighttime Moisturizer\", \"Cleansers\", \"Serum\", \"Exfoliants\", \"Eye Cream & Treatment\", \"Daytime Moisturizer\",\n",
    "                \"Sunscreen\", \"Toner & Face Mist\", \"Face Mask\", \"Acne & Blemish Treatment\", \"Lip Balm\", \"Retinol\",\n",
    "                \"Skin Lightener\", \"Oil Control Products\", \"Face Oil\", \"Vitamin C\", \"Lip Scrub\"]\n",
    "df2 = df2.loc[df2[\"product_category\"].isin(product_types)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ebc4ba",
   "metadata": {},
   "source": [
    "Now we need to create and clean up our size columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6058226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make and fill size_num column\n",
    "pattern = r'(\\d+(\\.\\d+)?)\\s*(fl\\.?\\s*oz\\.?|g\\.?|ml\\.?)'\n",
    "extracted_values = df2[\"product_name\"].str.extract(pattern, expand=True).fillna(\"\")\n",
    "df2[\"size_num\"] = extracted_values.iloc[:, 0]\n",
    "\n",
    "\n",
    "# make and fill size_unit column\n",
    "df2[\"size_unit\"] = df2[\"product_name\"]\n",
    "for index, row in df2.iterrows():\n",
    "    if \"g.\" in row[\"size_unit\"]:\n",
    "        row[\"size_unit\"] = \"grams\"\n",
    "    elif \"fl. oz\" in row[\"size_unit\"]:\n",
    "        row[\"size_unit\"] = \"fluid ounces\"\n",
    "    elif \"ml\" in row[\"size_unit\"]:\n",
    "        row[\"size_unit\"] = \"milliliters\"\n",
    "    else:\n",
    "        row[\"size_unit\"] = \"piece/other\"\n",
    "\n",
    "# make and fill a size column\n",
    "df2[\"size\"] = df2[\"size_num\"] + \" \" + df2[\"size_unit\"]\n",
    "df2[\"size\"] = df2[\"size\"].str.replace(\"grams\", \"g.\")\n",
    "df2[\"size\"] = df2[\"size\"].str.replace(\"milliliters\", \"ml\")\n",
    "df2[\"size\"] = df2[\"size\"].str.replace(\"fluid ounces\", \"fl. oz.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a7db1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary columns and make names uniform\n",
    "df2 = df2[[\"product_name\", \"product_category\", \"brand\", \"ingredients\", \"size\", \"price\", \"size_num\", \"size_unit\", \n",
    "          \"active_ingredients\", \"inactive_ingredients\", \"n_inactive_ingredient\", \"n_active_ingredient\"]]\n",
    "df2.columns = ['product_names', 'product_category', 'brand', 'ingredient', 'size', 'price', 'size_num', 'size_unit', \n",
    "               'active_ingredient', 'inactive_ingredient', 'n_inactive_ingredient', 'n_active_ingredient']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bff8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up active ingredient column\n",
    "df2[\"active_ingredient\"] = df2[\"active_ingredient\"].astype(str)\n",
    "df2[\"active_ingredient\"] = df2[\"active_ingredient\"].str.replace(\"'\", \"\")\n",
    "df2[\"active_ingredient\"] = df2[\"active_ingredient\"].str.replace(\"[\", \"\")\n",
    "df2[\"active_ingredient\"] = df2[\"active_ingredient\"].str.replace(\"]\", \"\")\n",
    "df2[\"active_ingredient\"] = df2[\"active_ingredient\"].str.replace(\"<strong>Active: </strong>\", \"\")\n",
    "df2[\"active_ingredient\"] = df2[\"active_ingredient\"].str.replace(\"<strong>Active Ingredients:</strong>\", \"\")\n",
    "df2[\"active_ingredient\"] = df2[\"active_ingredient\"].str.replace(\"Active Ingredients: \", \"\")\n",
    "df2[\"active_ingredient\"] = df2[\"active_ingredient\"].str.replace(\"\\n\", \"\")\n",
    "df2[\"active_ingredient\"] = df2[\"active_ingredient\"].str.replace(\"%\", \"\")\n",
    "df2[\"active_ingredient\"] = df2[\"active_ingredient\"].str.replace(\")\", \"\")\n",
    "df2[\"active_ingredient\"] = df2[\"active_ingredient\"].str.replace(\"(\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2eb899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join dataframes\n",
    "df = pd.concat([df1, df2]).reset_index().drop(columns = \"index\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fdf99b",
   "metadata": {},
   "source": [
    "The size of the product is often determined by the type of product it is – as in, moisturizer and sunscreen might be larger than serums. But there's a range even within these, depending on the brand. As a result, we'll keep size details in the dataframe without concern for collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ec4f51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn inactive ingredient column into lists\n",
    "df[\"inactive_ingredient\"] = df[\"inactive_ingredient\"].str.replace(\".\", \"\")\n",
    "df[\"inactive_ingredient\"] = df[\"inactive_ingredient\"].str.replace(\" \", \"\")\n",
    "df[\"inactive_ingredient\"] = df[\"inactive_ingredient\"].str.split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5611b29",
   "metadata": {},
   "source": [
    "Now we want to one-hot encode and stack our ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5d612c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ingredient counts\n",
    "ingredients_df = pd.get_dummies(pd.DataFrame(df[\"inactive_ingredient\"].tolist()).stack()).sum(level=0)\n",
    "ingredients_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aeda2a",
   "metadata": {},
   "source": [
    "Most products will have the same base ingredients. To ensure our model is determing price based on \"special\" ingredients or uncommon (but not impossibly rare) ingredients, we'll limit the set to ingredients that show up between 100 and 800 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edb6ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine most important ingredients\n",
    "ing_list = []\n",
    "for column in ingredients_df.columns:\n",
    "    if ingredients_df[column].sum() > 100 and ingredients_df[column].sum() < 800:\n",
    "        ing_list.append(column)\n",
    "\n",
    "ing_list = [ele.strip() for ele in ing_list]\n",
    "\n",
    "# get rid of empty string\n",
    "ing_list = ing_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7357bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe from columns in ing_list\n",
    "ingredients_df = ingredients_df.rename(columns=lambda x: x.strip())\n",
    "clean_ing = ingredients_df[ing_list]\n",
    "\n",
    "# concat dataframes\n",
    "df_ing = pd.concat([df, clean_ing], axis = 1)\n",
    "df_ing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2b6621",
   "metadata": {},
   "source": [
    "Now our dataframe is looking more manageable with only 205 columns as opposed to nearly 13,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02ef45ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate columns\n",
    "df_ing = df_ing.groupby(axis=1, level=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fa58f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure our float columns are floats\n",
    "df_ing[\"price\"] = df_ing[\"price\"].astype(float)\n",
    "df_ing[\"size_num\"] = df_ing[\"size_num\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "df_ing[\"size_num\"] = df_ing[\"size_num\"].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6870c01f",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Let's build the preprocessor our XGBoost regressor will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd5348e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ord_cols\n",
    "ord_cols = [\"brand\", \"product_category\", \"size_unit\"]\n",
    "\n",
    "#num_cols\n",
    "num_cols = [\"size_num\", \"n_active_ingredient\", \"n_inactive_ingredient\"]\n",
    "ingredients = df_ing.columns.to_list()[:-12]\n",
    "for ing in ingredients:\n",
    "    num_cols.append(ing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ce0a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_transformer = Pipeline(steps = [\n",
    "    ('ordimputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('target_enc', TargetEncoder()),\n",
    "    (\"scaler\", StandardScaler())])\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"ord\", ordinal_transformer, ord_cols),\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1adaaee",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "Time for the train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0611809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_ing[\"price\"]\n",
    "X = df_ing.drop(columns = [\"price\", \"active_ingredient\", \"inactive_ingredient\", \"ingredient\", \"product_names\", \"size\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba8dc216",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e534a6d2",
   "metadata": {},
   "source": [
    "Just to check, let's see what the MAE is before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fce5085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine baseline error\n",
    "mae_train = mean_absolute_error(y_train, np.ones([y_train.shape[0],])* y_train.median())\n",
    "mae_test = mean_absolute_error(y_test, np.ones([y_test.shape[0],])* y_train.median())\n",
    "\n",
    "print(\"MAE\")\n",
    "print(f\"train: {mae_train:.3f}\") \n",
    "print(f\"test: {mae_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bce7f8",
   "metadata": {},
   "source": [
    "Right now, we have a mean absolute error of almsot $30 for both train and test. This is what we'll want to beat with our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d8e92a",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "## Model 1 - Simple Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918dc631",
   "metadata": {},
   "source": [
    "Before diving into neural networks, let's see how our tried and tested XGBoost does with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bf08972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate, fit and run model\n",
    "X_train_ct = preprocessor.fit_transform(X_train, y_train)\n",
    "boost = XGBRegressor()\n",
    "boost.fit(X_train_ct, y_train)\n",
    "pred_train_boost = boost.predict(X_train_ct)\n",
    "\n",
    "X_test_ct = preprocessor.transform(X_test)\n",
    "pred_test_boost = boost.predict(X_test_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "402a039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine error\n",
    "mae_train = mean_absolute_error(y_train, pred_train_boost)\n",
    "mae_test = mean_absolute_error(y_test, pred_test_boost)\n",
    "\n",
    "print(\"MAE: \")\n",
    "print(f\"train: {mae_train:.3f}\")\n",
    "print(f\"test: {mae_test: .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5ab0658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions against true prices\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(y_test, pred_test_boost)\n",
    "plt.plot([0,300],[0,300],color=\"r\")\n",
    "plt.axis(\"Equal\")\n",
    "plt.xlim(0,300)\n",
    "plt.ylim(0,300)\n",
    "plt.xlabel(\"True Price\")\n",
    "plt.ylabel(\"Predicted Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d056476",
   "metadata": {},
   "source": [
    "Not bad! Only about 13 dollars off on average. There is a pretty big range in the products, as we saw above, with some going as high as 700 dollars. 13 dollars seems pretty good with that in mind. Looking at the predictions when plotted, they seem to be clustered pretty well around the equal line though a few seem either overpriced or underpriced based on the model's determination.\n",
    "\n",
    "Since our simple regressor did pretty well. Let's try to do a grid search to further tune the model and see how that does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68416487",
   "metadata": {},
   "source": [
    "## Model 2 - Grid Search\n",
    "\n",
    "We'll tune the model with a range of learning rates, estimators, and depths to see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d99a6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter grid\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.03, 0.05, 0.07],\n",
    "    \"n_estimators\": [500, 750, 1000],\n",
    "    \"max_depth\": [5, 7, 9]\n",
    "}\n",
    "\n",
    "\n",
    "# instantiate and fit\n",
    "boost = XGBRegressor()\n",
    "grid_search = GridSearchCV(boost, param_grid, cv=5, scoring=\"neg_mean_absolute_error\")\n",
    "grid_search.fit(X_train_ct, y_train)\n",
    "\n",
    "# return best parameters and the best score\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n",
    "\n",
    "# get predictions off best model\n",
    "best_boost = grid_search.best_estimator_\n",
    "pred_test_boost_gs = best_boost.predict(X_test_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b81e3594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions against true prices\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(y_test, pred_test_boost_gs)\n",
    "plt.plot([0,300],[0,300],color=\"r\")\n",
    "plt.axis(\"Equal\")\n",
    "plt.xlim(0,300)\n",
    "plt.ylim(0,300)\n",
    "plt.xlabel(\"True Price\")\n",
    "plt.ylabel(\"Predicted Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c043dafd",
   "metadata": {},
   "source": [
    "## Model 3: Stacked Regressor\n",
    "\n",
    "We can go further with regressors and stack them together to see if that improves our score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa03f932",
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = XGBRegressor()\n",
    "rf = RandomForestRegressor()\n",
    "ridge = Ridge()\n",
    "lasso = Lasso()\n",
    "svr = SVR(kernel='linear')\n",
    "\n",
    "stack = StackingCVRegressor(regressors=(boost, rf, ridge, lasso, svr),\n",
    "                            meta_regressor=boost, cv=5,\n",
    "                            use_features_in_secondary=True,\n",
    "                            store_train_meta_features=True,\n",
    "                            shuffle=False,\n",
    "                            random_state=42)\n",
    "\n",
    "stack.fit(X_train_ct, y_train)\n",
    "pred_stack = stack.predict(X_test_ct)\n",
    "mae_test = mean_absolute_error(y_test, pred_stack)\n",
    "\n",
    "print(\"MAE: \")\n",
    "print(f\"test: {mae_test: .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f72830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions against true prices\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(y_test, pred_stack)\n",
    "plt.plot([0,300],[0,300],color=\"r\")\n",
    "plt.axis(\"Equal\")\n",
    "plt.xlim(0,300)\n",
    "plt.ylim(0,300)\n",
    "plt.xlabel(\"True Price\")\n",
    "plt.ylabel(\"Predicted Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d18583",
   "metadata": {},
   "source": [
    "Not a lot of improvement from our untuned XGBoost. On to the neural networks to see if they can do more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb0c410",
   "metadata": {},
   "source": [
    "## Model 4 - Neural Network, 3 Hidden Layers\n",
    "\n",
    "For the first neural network, we'll start simple with 3 hidden layers and two dropouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "220bbe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ct = preprocessor.fit_transform(X_train, y_train)\n",
    "X_test_ct = preprocessor.transform(X_test)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "model4 = Sequential([\n",
    "    Dense(520, kernel_initializer='normal', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(480, kernel_initializer='normal', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(256, kernel_initializer='normal', activation='relu'),\n",
    "    Dense(1, kernel_initializer='normal', activation='linear')\n",
    "  ])\n",
    "\n",
    "mae_k = MeanAbsoluteError()\n",
    "model4.compile(loss=mae_k, optimizer=Adam(learning_rate=learning_rate), metrics=[mae_k])\n",
    "\n",
    "model_4 = model4.fit(X_train_ct, y_train, epochs=100, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27bfaafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model4.predict(X_train_ct)\n",
    "y_test_pred = model4.predict(X_test_ct)\n",
    "\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"MAE: \")\n",
    "print(f\"train: {mae_train:.3f}\")\n",
    "print(f\"test: {mae_test: .3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4fc91f",
   "metadata": {},
   "source": [
    "The Neural Network didn't improve upon the XGBoost regressor MAE score. In fact it's gone up by a point, more than our stacked model or the gridsearch tuned XGBoost. Maybe more layers will help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b03be7",
   "metadata": {},
   "source": [
    "## Model 4 - Neural Network, 5 Hidden Layers¶\n",
    "\n",
    "For this model, we'll add more hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d998789",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "model5 = Sequential([\n",
    "    Dense(520, kernel_initializer='normal', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(480, kernel_initializer='normal', activation='relu'),\n",
    "    Dense(480, kernel_initializer='normal', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(256, kernel_initializer='normal', activation='relu'),\n",
    "    Dense(256, kernel_initializer='normal', activation='relu'),\n",
    "    Dense(1, kernel_initializer='normal', activation='linear')\n",
    "  ])\n",
    "\n",
    "mae_k = MeanAbsoluteError()\n",
    "model5.compile(loss=mae_k, optimizer=Adam(learning_rate=learning_rate), metrics=[mae_k])\n",
    "\n",
    "model_5 = model5.fit(X_train_ct, y_train, epochs=100, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "413aaf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model5.predict(X_train_ct)\n",
    "y_test_pred = model5.predict(X_test_ct)\n",
    "\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"MAE: \")\n",
    "print(f\"train: {mae_train:.3f}\")\n",
    "print(f\"test: {mae_test: .3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13f5197",
   "metadata": {},
   "source": [
    "Still no dice. Looks like more layers hurt the MAE score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9f8dbc",
   "metadata": {},
   "source": [
    "## Model 6 - Neural Network, 10 Hidden Layers¶\n",
    "\n",
    "For the final neural network, we'll try 10 hidden layers to see if that improves the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aae2f5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "model6 = Sequential([\n",
    "    Dense(160, kernel_initializer='normal', activation='relu'),\n",
    "    Dense(160, kernel_initializer='normal', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(480, kernel_initializer='normal', activation='relu'),\n",
    "    Dense(480, kernel_initializer='normal', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(520, kernel_initializer='normal', activation='relu'),\n",
    "    Dense(520, kernel_initializer='normal', activation='relu'),\n",
    "    Dense(760, kernel_initializer='normal', activation='relu'),\n",
    "    Dense(760, kernel_initializer='normal', activation='relu'),\n",
    "    Dense(760, kernel_initializer='normal', activation='relu'),\n",
    "    Dense(256, kernel_initializer='normal', activation='relu'),\n",
    "    Dense(256, kernel_initializer='normal', activation='relu'),\n",
    "    Dense(1, kernel_initializer='normal', activation='linear')\n",
    "  ])\n",
    "\n",
    "model6.compile(loss=mae_k, optimizer=Adam(learning_rate=learning_rate), metrics=[mae_k])\n",
    "\n",
    "model_6 = model6.fit(X_train_ct, y_train, epochs=100, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77362740",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model6.predict(X_train_ct)\n",
    "y_test_pred = model6.predict(X_test_ct)\n",
    "\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"MAE: \")\n",
    "print(f\"train: {mae_train:.3f}\")\n",
    "print(f\"test: {mae_test: .3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa1a92",
   "metadata": {},
   "source": [
    "This is our worst model so far. Added layers did nothing to improve our score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36089882",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Using our baseline XGBoost regressor, let's look at the most important features to see if that will help Inner Beauty Inc. understand where to focus their research and development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94bfa1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "872bd86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember model\n",
    "X_train_ct = preprocessor.fit_transform(X_train, y_train)\n",
    "boost = XGBRegressor()\n",
    "boost.fit(X_train_ct, y_train)\n",
    "\n",
    "# plot feature importances\n",
    "features_df = pd.DataFrame()\n",
    "features_df[\"feature\"] = X_train.columns\n",
    "features_df[\"importance\"] = boost.feature_importances_\n",
    "features_df = features_df.sort_values(\"importance\", ascending=False)\n",
    "plt.figure(figsize=(5,12))\n",
    "sns.barplot(x = \"importance\", y = \"feature\", data=features_df.iloc[:15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd2f749",
   "metadata": {},
   "source": [
    "Feature importance seems negligible here since even the most influential feature, the ingredient Urea, is less than 0.1. This might mean that ingredients matter little individually, but mean a lot in groups. Brand and product category don't rank at all, which means they don't have as much influence on price as we originally thought. Size does figure in this chart, which makes sense–a larger product might be more expensive in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f857b",
   "metadata": {},
   "source": [
    "## Over and Under Pricing\n",
    "\n",
    "It would help Inner Beauty to understand which brands our model considers overpriced or underpriced based in the features. We'll start by creating a dataframe of predicted prices against real prices and then break it down from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a717eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of prices and predictions\n",
    "df_prices = df_ing.loc[y_test.index.to_list()]\n",
    "df_prices = df_prices[[\"product_names\", \"product_category\", \"brand\", \"ingredient\"]]\n",
    "df_prices[\"real_price\"] = y_test\n",
    "df_prices[\"predicted_price\"] = pred_test_boost\n",
    "df_prices[\"difference\"] = (df_prices[\"real_price\"] - df_prices[\"predicted_price\"]).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94af29f3",
   "metadata": {},
   "source": [
    "Overpriced we'll define as more than a $20 difference between real and predicted price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c93c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find most overpriced brands\n",
    "overpriced = df_prices.loc[df_prices[\"difference\"] > 20]\n",
    "overpriced.brand.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f952c2",
   "metadata": {},
   "source": [
    "La Prairie makes sense as the most overpriced brand as it sells a 1700 dollar perfume among its other products. Its Skin Caviar Luxe Cream alone is almost 600 dollars. Perricone MD Cosmeceuticals and Dr. Brandt are interesting as they are more mid-line brands. \n",
    "\n",
    "Underpriced would be any products our model thinks should be more expensive than it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "862a5285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find most underpriced brands\n",
    "underpriced = df_prices.loc[df_prices[\"difference\"] < 0]\n",
    "underpriced.brand.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80884685",
   "metadata": {},
   "source": [
    "We have a lot more products in our dataset from the underpriced brands than the overpriced ones. This could be because the overpriced brands have smaller, more exclusive catalogs. Neutrogena and L'Oreal Paris are a drugstore staples, it makes sense they would be at the top of the underpriced list. Shiseido appears on both lists, interestingly enough. Though it has more underpriced products than overpriced ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c1064c",
   "metadata": {},
   "source": [
    "# Dupe Machine\n",
    "\n",
    "In order to easily understand the paralell products in the dataset, here's a function that returns cheaper products based on inputs. It finds similar products based on ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c94f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-essential columns\n",
    "features = df_ing.copy()\n",
    "features = features.drop(columns = [\"product_names\", \"size\", \"size_num\", \"size_unit\", \"brand\", \"active_ingredient\", \n",
    "                        \"inactive_ingredient\", \"ingredient\"])\n",
    "features = pd.get_dummies(features)\n",
    "\n",
    "# fit KNN model\n",
    "model = NearestNeighbors(n_neighbors=2, algorithm='ball_tree')\n",
    "model.fit(features)\n",
    "dist, idlist = model.kneighbors(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ecd22412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function\n",
    "def SuperDuper(product_name):\n",
    "    product_list = []\n",
    "    product_id = df_ing[df_ing[\"product_names\"] == product_name].index\n",
    "    product_id = product_id[0]\n",
    "    product_price = df_ing[\"price\"].iloc[product_id] \n",
    "    for newid in idlist[product_id]:\n",
    "        name = df_ing.loc[newid].product_names\n",
    "        brand = df_ing.loc[newid].brand\n",
    "        price = df_ing.loc[newid].price\n",
    "        price_diff = product_price - price\n",
    "        if name == product_name:\n",
    "            pass\n",
    "        elif price < product_price:\n",
    "            product_list.append(f\"{name} from {brand}, ${price: .2f}, saving ${price_diff: .2f}\")\n",
    "        \n",
    "    return product_list   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a62e47fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "SuperDuper(\"Naturals Acne Spot Treatment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b9fc1e",
   "metadata": {},
   "source": [
    "Let's save all of this information for the Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d1f1c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframes and pickles for SuperDuper streamlit app\n",
    "products_data = df_ing[[\"brand\", \"product_names\", \"price\"]]\n",
    "products_data.to_csv(r\"data/products_data.csv\")\n",
    "features.to_pickle(\"data/features.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6238c4a",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "My recommendation for Inner Beauty Inc is to dupe the overpriced brands with products that undercut their product lines and price them higher than the overpriced brands do to ensure strong margins.\n",
    "\n",
    "For next steps, it would be useful to go deeper into the ingredients and understand what combinations of ingredients yield the highest prices. Also, packaging signals the quality of the product or at least the price range it operates in. Using image classification might be useful to further the analysis. Finally, Inner Beauty Inc needs to know what product categories to focus on. That would add another element to the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8422958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "middle-earth",
   "language": "python",
   "name": "middle-earth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
